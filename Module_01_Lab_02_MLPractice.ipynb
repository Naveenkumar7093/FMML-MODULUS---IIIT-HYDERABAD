{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naveenkumar7093/FMML-MODULUS---IIIT-HYDERABAD/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca09195c-5f0f-44ad-ba88-93752b7335f1"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4b21f4-707b-418d-cb1d-cad85478b2a7"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e95ea6f-c085-4753-e972-ada1f5bd1aa4"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e56cf5e-6fed-4558-cad9-93af4a326779"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0905f162-0eab-43db-bc0f-a80fff162da5"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f82edc3-639e-4e17-9465-bf8189d5fad8"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58baac5b-ae11-45ab-f8a6-cb98e6d436ce"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st question ans"
      ],
      "metadata": {
        "id": "lwrNfCMfigc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, averaging the validation accuracy across multiple splits can give you more consistent and reliable results when evaluating the performance of a machine learning model. This technique is commonly referred to as \"cross-validation.\" Cross-validation helps address the issue of variability in the performance metrics that can arise from a single random split of your data into training and validation sets.\n",
        "\n",
        "Here's how cross-validation works and why it provides more consistent results:\n",
        "\n",
        "Data Splitting: Instead of performing a single random split of your dataset into a training set and a validation set, cross-validation involves dividing the data into multiple subsets or \"folds.\" Typically, a common choice is k-fold cross-validation, where the data is split into k equally sized subsets.\n",
        "\n",
        "Training and Evaluation: You then train and evaluate your model k times, each time using a different fold as the validation set and the remaining folds as the training set. This means that every data point in your dataset gets a chance to be in the validation set at least once.\n",
        "\n",
        "Averaging: After running the model k times, you calculate the validation accuracy (or other performance metric) for each run. Finally, you calculate the average of these k performance metrics to obtain a more stable and representative estimate of your model's performance.\n",
        "\n",
        "By averaging the results over multiple splits, cross-validation helps you mitigate the effects of random variations that might occur when a single random split is used. It provides a more robust assessment of how well your model generalizes to new, unseen data.\n",
        "\n",
        "Common choices for k include 5-fold and 10-fold cross-validation, but the appropriate value may vary depending on your dataset size and specific problem. In some cases, you might also consider other variations of cross-validation, such as stratified k-fold cross-validation or leave-one-out cross-validation.\n",
        "\n",
        "In summary, averaging validation accuracy (or other performance metrics) across multiple splits using cross-validation is a good practice to obtain more consistent and reliable estimates of your model's performance, helping you make better-informed decisions about model selection and hyperparameter tuning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Send a message\n",
        "\n",
        "\n",
        "Free Research Preview. ChatGPT may produce"
      ],
      "metadata": {
        "id": "EIRvmhgIijgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 nd question ans"
      ],
      "metadata": {
        "id": "MQkUK2T0ji4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation, while providing a more stable and reliable estimate of a model's performance compared to a single random split, does not directly give you a more accurate estimate of test accuracy. The reason for this is that cross-validation is primarily used to estimate how well your model generalizes to unseen data within your training dataset, not to estimate performance on a completely independent test dataset.\n",
        "\n",
        "Here's a breakdown of the distinctions:\n",
        "\n",
        "Cross-validation: Cross-validation estimates how well your model generalizes to different subsets of your training data. It helps you assess the model's consistency and robustness to variations in the data by averaging performance metrics across multiple folds. It provides a good sense of how your model might perform on similar data within the same dataset.\n",
        "\n",
        "Test Accuracy: Test accuracy, on the other hand, measures how well your model performs on entirely new and unseen data that was not used during training or cross-validation. This is typically a better representation of how your model will perform in real-world applications, where you encounter data from sources you haven't seen before.\n",
        "\n",
        "In practice, you should use cross-validation to tune hyperparameters, select models, and assess their generalization capabilities within your training dataset. Once you have a final model and set of hyperparameters you are satisfied with, you should evaluate its performance on a separate, independent test dataset to get an accurate estimate of how it will perform in real-world scenarios.\n",
        "\n",
        "So, while cross-validation is a valuable tool for model selection and hyperparameter tuning, it doesn't replace the need for a dedicated test dataset to provide an accurate estimate of test accuracy. The test dataset serves as a critical step in evaluating the model's real-world performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Send a message\n",
        "\n",
        "\n",
        "Free Research Preview. ChatGPT may produce inacc"
      ],
      "metadata": {
        "id": "XMHoNJq-jlKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 rd question ans"
      ],
      "metadata": {
        "id": "aOrbBX9KkBwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of iterations, also referred to as the number of training epochs, can have a significant effect on the estimate of a model's performance during training and evaluation. In many cases, increasing the number of iterations can lead to a better estimate of the model's performance, but there are caveats to consider:\n",
        "\n",
        "Underfitting and Overfitting: The number of iterations plays a crucial role in training a model. If you have too few iterations (underfitting), the model may not have sufficient opportunities to learn from the data, and its performance on both the training and validation sets may be subpar. Conversely, if you have too many iterations (overfitting), the model may memorize the training data and perform poorly on new, unseen data.\n",
        "\n",
        "Learning Curves: Typically, as you increase the number of iterations, you may observe changes in the learning curves. In the beginning, the training and validation performance may improve as the model learns from the data. However, after a certain point, the validation performance might start to degrade due to overfitting, while the training performance continues to improve. The point where the validation performance begins to degrade is a good indicator of when to stop increasing the iterations.\n",
        "\n",
        "Computational Resources: Training a model with a high number of iterations can be computationally expensive, especially for deep learning models with many parameters. You should balance the computational resources available with the benefits gained from additional iterations.\n",
        "\n",
        "Early Stopping: To mitigate overfitting and make better use of computational resources, you can implement techniques like early stopping. Early stopping involves monitoring the validation performance during training and stopping training when the validation performance starts to degrade. This prevents the model from overfitting.\n",
        "\n",
        "In summary, the number of iterations can impact the estimate of a model's performance. However, the relationship is not linear, and there's often an optimal number of iterations that balances underfitting and overfitting. It's essential to monitor learning curves, use techniques like early stopping, and perform hyperparameter tuning to find the right number of iterations for your specific problem and dataset. Simply increasing the number of iterations indefinitely does not necessarily lead to a better estimate, as it can lead to overfitting and increased computational costs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Send a message\n",
        "\n",
        "\n",
        "Free Research Preview. ChatGPT may produce inaccurate informa"
      ],
      "metadata": {
        "id": "CSjW3vGAkEgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 th question ans"
      ],
      "metadata": {
        "id": "oDYv1l8zkIHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of iterations can help to some extent when dealing with very small training or validation datasets, but it is not a guaranteed solution to overcome the limitations associated with small datasets. When working with limited data, there are several considerations to keep in mind:\n",
        "\n",
        "Overfitting: If you have a very small training dataset and you increase the number of iterations significantly, your model might start to overfit the training data. Overfitting occurs when the model becomes too complex and starts to memorize the training examples rather than learning to generalize from them. This can lead to poor performance on unseen data, including the validation dataset.\n",
        "\n",
        "Data Augmentation: Instead of solely relying on more iterations, you can augment your small dataset. Data augmentation techniques involve creating new training examples by applying transformations (e.g., rotations, translations, or flips) to your existing data. This can effectively increase the effective size of your training dataset and help the model learn more robust features.\n",
        "\n",
        "Regularization: Use regularization techn"
      ],
      "metadata": {
        "id": "VCpWBry5kKdU"
      }
    }
  ]
}